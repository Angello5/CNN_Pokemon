{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc0c48f-d18d-499c-a310-f9bf63473049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 23:12:18.053074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738728738.075903   54393 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738728738.082359   54393 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-04 23:12:18.101527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmt\n",
    "from tensorflow.keras import Sequential\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import zipfile\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError\n",
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a104bb-9735-4a99-ba2d-b335cd28f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibles: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b04d07d-08c4-4f1e-8569-9cd56070ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Preprocesamiento del Dataset\n",
    "# Definir el directorio donde se encuentran las imágenes del dataset\n",
    "with zipfile.ZipFile('/home/pibezx/Documents/CNN/pokemon.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('pokemos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297346d8-4fc9-4424-9359-ed247f3921b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"pokemos/PokemonData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebd9ad4-cc7e-4e3b-8086-ce99a38aa148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imágenes después de filtrar: 11949\n",
      "Número de imágenes cargadas inicialmente: 11949\n",
      "Número de etiquetas cargadas inicialmente: 11949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738728745.651568   54393 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3893 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:0b:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Preprocesamiento del Dataset\n",
    "# Parámetros de preprocesamiento\n",
    "IMG_SIZE = (224, 224)  # Tamaño al que se redimensionarán las imágenes\n",
    "BATCH_SIZE = 32  # Tamaño del lote para preprocesamiento\n",
    "\n",
    "# Listar todas las imágenes en el directorio\n",
    "image_files = []\n",
    "labels = []\n",
    "for root, _, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "            image_files.append(os.path.join(root, file))\n",
    "            labels.append(os.path.basename(root))\n",
    "\n",
    "# Convertir listas de archivos y etiquetas a tensores\n",
    "image_files = tf.constant(image_files)\n",
    "labels = tf.constant(labels)\n",
    "\n",
    "# Filtrar clases con al menos 2 ejemplos\n",
    "label_counts = Counter(labels.numpy())\n",
    "valid_labels = [label for label, count in label_counts.items() if count >= 2]\n",
    "filtered_image_files = []\n",
    "filtered_labels = []\n",
    "for img, label in zip(image_files.numpy(), labels.numpy()):\n",
    "    if label in valid_labels:\n",
    "        filtered_image_files.append(img)\n",
    "        filtered_labels.append(label)\n",
    "\n",
    "image_files = tf.constant(filtered_image_files)\n",
    "labels = tf.constant(filtered_labels)\n",
    "\n",
    "print(f\"Total de imágenes después de filtrar: {len(image_files)}\")\n",
    "\n",
    "\n",
    "# Codificar las etiquetas para que sean valores enteros\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels.numpy())\n",
    "\n",
    "# Convertir etiquetas a tensor de TensorFlow\n",
    "labels = tf.constant(labels)\n",
    "\n",
    "# Crear un dataset de TensorFlow con las rutas de las imágenes y etiquetas\n",
    "def parse_function(filename, label):\n",
    "    # Leer el archivo de imagen\n",
    "    img = tf.io.read_file(filename)\n",
    "    # Decodificar la imagen\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Redimensionar la imagen\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    # Normalizar los valores de los píxeles entre 0 y 1\n",
    "    img = img / 255.0\n",
    "    return img, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_files, labels))\n",
    "dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(f\"Número de imágenes cargadas inicialmente: {len(image_files)}\")\n",
    "print(f\"Número de etiquetas cargadas inicialmente: {len(labels)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c343014-5ffb-4261-8509-ec4be152adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividiendo el dataset en conjuntos de entrenamiento, validación y prueba...\n",
      "Número total de muestras en el dataset: 11949\n",
      "Tamaño del conjunto de Entrenamiento: 8364\n",
      "Tamaño del conjunto de Validación: 2389\n",
      "Tamaño del conjunto de Prueba: 1196\n",
      "Tamaño del conjunto de Entrenamiento (número de lotes): 262\n",
      "Tamaño del conjunto de Validación (número de lotes): 75\n",
      "Tamaño del conjunto de Prueba (número de lotes): 38\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: División del Dataset\n",
    "print(\"Dividiendo el dataset en conjuntos de entrenamiento, validación y prueba...\")\n",
    "dataset = dataset.shuffle(len(image_files), reshuffle_each_iteration=False)\n",
    "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "print(f\"Número total de muestras en el dataset: {dataset_size}\")\n",
    "train_size = int(0.7 * len(image_files))\n",
    "val_size = int(0.2 * len(image_files))\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "print(f\"Tamaño del conjunto de Entrenamiento: {train_size}\")\n",
    "print(f\"Tamaño del conjunto de Validación: {val_size}\")\n",
    "print(f\"Tamaño del conjunto de Prueba: {test_size}\")\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_test_dataset = dataset.skip(train_size)\n",
    "val_dataset = val_test_dataset.take(val_size)\n",
    "test_dataset = val_test_dataset.skip(val_size)\n",
    "\"\"\"\n",
    "# Definir la función de data augmentation\n",
    "def augment(image, label):\n",
    "    # Aplicar transformaciones aleatorias\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n",
    "    image = tf.image.random_hue(max_delta=0.1)\n",
    "    # Asegurarse de que la imagen sigue en el rango [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    return image, label\n",
    "\n",
    "# Aplicar data augmentation solo al conjunto de entrenamiento\n",
    "train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\"\"\"\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Tamaño del conjunto de Entrenamiento (número de lotes): {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
    "print(f\"Tamaño del conjunto de Validación (número de lotes): {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
    "print(f\"Tamaño del conjunto de Prueba (número de lotes): {tf.data.experimental.cardinality(test_dataset).numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170c16c4-65b2-4bed-ab98-71c4533b4e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54393/3739781872.py:5: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando el modelo MobileNetV2 para entrenamiento...\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Entrenamiento del Modelo con MobileNetV2\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "print(\"Configurando el modelo MobileNetV2 para entrenamiento...\")\n",
    "input_tensor = Input(shape=(224, 224, 3))  # Usando 3 canales ya que las imágenes son ahora \"RGB\"\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76965e3a-d76e-4318-a3b2-ddee2b09ca1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Functional.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# -----------\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Paso 4: Envolver la base en prune_low_magnitude\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# -----------\u001b[39;00m\n\u001b[1;32m     26\u001b[0m prune_low_magnitude \u001b[38;5;241m=\u001b[39m tfmt\u001b[38;5;241m.\u001b[39msparsity\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mprune_low_magnitude\n\u001b[0;32m---> 27\u001b[0m pruned_base_model \u001b[38;5;241m=\u001b[39m prune_low_magnitude(base_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpruning_params)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# -----------\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Paso 5: Construir el modelo Sequential con la base podada y capas finales\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# -----------\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m     33\u001b[0m     pruned_base_model,               \u001b[38;5;66;03m# Base MobileNetV2 podada\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     GlobalAveragePooling2D(),\n\u001b[1;32m     35\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     36\u001b[0m     Dense(\u001b[38;5;28mlen\u001b[39m(valid_labels), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m ])\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn_env/lib/python3.11/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:74\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_FAILURE_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn_env/lib/python3.11/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:69\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     results \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_SUCCESS_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn_env/lib/python3.11/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/prune.py:216\u001b[0m, in \u001b[0;36mprune_low_magnitude\u001b[0;34m(to_prune, pruning_schedule, block_size, block_pooling_type, pruning_policy, sparsity_m_by_n, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m pruning_wrapper\u001b[38;5;241m.\u001b[39mPruneLowMagnitude(to_prune, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`prune_low_magnitude` can only prune an object of the following \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtypes: keras.models.Sequential, keras functional model, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    219\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.layers.Layer, list of keras.layers.Layer. You passed \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    220\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man object of type: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mto_prune\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    221\u001b[0m   )\n",
      "\u001b[0;31mValueError\u001b[0m: `prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Functional."
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# -----------\n",
    "# Paso 2: (Opcional) Congelar las capas de la base\n",
    "# -----------\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# -----------\n",
    "# Paso 3: Definir parámetros para pruning\n",
    "# -----------\n",
    "epochs = 40\n",
    "steps_per_epoch = 100  # Ajusta según tu dataset\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmt.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=0.6,\n",
    "        begin_step=0,\n",
    "        end_step=steps_per_epoch * epochs\n",
    "    )\n",
    "}\n",
    "\n",
    "# -----------\n",
    "# Paso 4: Envolver la base en prune_low_magnitude\n",
    "# -----------\n",
    "prune_low_magnitude = tfmt.sparsity.keras.prune_low_magnitude\n",
    "pruned_base_model = prune_low_magnitude(base_model, **pruning_params)\n",
    "\n",
    "# -----------\n",
    "# Paso 5: Construir el modelo Sequential con la base podada y capas finales\n",
    "# -----------\n",
    "model = Sequential([\n",
    "    pruned_base_model,               # Base MobileNetV2 podada\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(valid_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "# -----------\n",
    "# Paso 6: Compilar\n",
    "# -----------\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -----------\n",
    "# Paso 7: Entrenar con callbacks para pruning y early stopping\n",
    "# -----------\n",
    "callbacks = [\n",
    "    tfmt.sparsity.keras.UpdatePruningStep(),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\"\"\"\n",
    "# Parámetros para pruning\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmt.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0, \n",
    "        final_sparsity=0.6, \n",
    "        begin_step=0, \n",
    "        end_step=steps_per_epoch * epochs\n",
    "    )\n",
    "}\n",
    "prune_low_magnitude = tfmt.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# === Paso 1: Envolver la base de MobileNetV2 con prune_low_magnitude ===\n",
    "pruned_model_base = prune_low_magnitude(base_model, **pruning_params)\n",
    "\n",
    "# === Paso 2: Construir la parte final de la red usando la salida de la red podada ===\n",
    "x = pruned_model_base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(len(valid_labels), activation='softmax')(x)\n",
    "\n",
    "# === Paso 3: Crear el modelo que realmente usaremos, con pruning incluido ===\n",
    "pruned_model = Model(inputs=pruned_model_base.input, outputs=predictions)\n",
    "\n",
    "# Compilar\n",
    "pruned_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "pruning_callback = tfmt.sparsity.keras.UpdatePruningStep()\n",
    "\n",
    "# Entrenar\n",
    "pruned_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, pruning_callback]\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1aeda-2fe8-4236-b839-d2ad13a7461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = pruned_model.evaluate(test_dataset)\n",
    "\n",
    "# Imprimir la precisión en el conjunto de prueba\n",
    "print(f\"Precisión en el conjunto de prueba: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6762165-a812-43fd-aee4-a3fc26b06be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Obtener las etiquetas verdaderas y predichas\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    predictions = pruned_model.predict(images)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(predicted_labels)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Obtener los nombres de las clases y decodificarlos si es necesario\n",
    "class_names = label_encoder.classes_\n",
    "class_names = [name.decode('utf-8') if isinstance(name, bytes) else name for name in class_names]\n",
    "\n",
    "# Número máximo de clases a visualizar\n",
    "max_classes = 20\n",
    "\n",
    "# Índices de las clases que deseas visualizar (por ejemplo, las primeras 20)\n",
    "clases_mostrar = np.arange(max_classes)\n",
    "\n",
    "# Obtener los nombres de las clases de la lista ya decodificada\n",
    "class_names_mostrar = [class_names[i] for i in clases_mostrar]\n",
    "\n",
    "\n",
    "# Normalizar la matriz de confusión completa\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Extraer la submatriz para las clases seleccionadas\n",
    "cm_mostrar = cm_norm[np.ix_(clases_mostrar, clases_mostrar)]\n",
    "\n",
    "# Visualizar la matriz de confusión normalizada para las clases seleccionadas\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_mostrar, annot=False, cmap='Blues', xticklabels=class_names_mostrar, yticklabels=class_names_mostrar)\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.title('Matriz de Confusión Normalizada (20 Clases Seleccionadas)')\n",
    "plt.show()\n",
    "\n",
    "# Generar el reporte de clasificación para las clases seleccionadas\n",
    "report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    labels=clases_mostrar, \n",
    "    target_names=class_names_mostrar\n",
    ")\n",
    "\n",
    "print(\"Reporte de Clasificación para las Clases Seleccionadas:\\n\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da2dc4-5e27-455c-a875-4e8d0f347f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_predicciones_en_prueba(dataset, num_images=6):\n",
    "    # Crear un iterador del dataset\n",
    "    dataset_iter = iter(dataset.unbatch().shuffle(1000))\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_images):\n",
    "        # Obtener la imagen y la etiqueta verdadera\n",
    "        image, label = next(dataset_iter)\n",
    "        # Expandir dimensiones para predecir\n",
    "        image_expanded = tf.expand_dims(image, axis=0)\n",
    "        # Hacer la predicción\n",
    "        pred_probs = pruned_model.predict(image_expanded)\n",
    "        pred_label = np.argmax(pred_probs, axis=1)[0]\n",
    "        # Obtener los nombres de las clases\n",
    "        true_class_name = label_encoder.inverse_transform([label.numpy()])[0]\n",
    "        pred_class_name = label_encoder.inverse_transform([pred_label])[0]\n",
    "        # Mostrar la imagen con las etiquetas\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'Verdadero: {true_class_name}\\nPredicción: {pred_class_name}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Mostrar predicciones en imágenes del conjunto de prueba\n",
    "mostrar_predicciones_en_prueba(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cc76b-e924-4572-ab25-9f446ca08774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_y_preprocesar_imagen(ruta_imagen, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Carga una imagen desde ruta_imagen, la decodifica, la redimensiona\n",
    "    y la normaliza a [0.0, 1.0].\n",
    "    Devuelve un tf.Tensor con la forma (224, 224, 3).\n",
    "    \"\"\"\n",
    "    # 1. Leer la imagen en binario\n",
    "    img = tf.io.read_file(ruta_imagen)\n",
    "\n",
    "    # 2. Decodificar la imagen (asumiendo JPG/PNG con 3 canales)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "    # 3. Redimensionar la imagen\n",
    "    img = tf.image.resize(img, img_size)\n",
    "\n",
    "    # 4. Normalizar la imagen [0,1]\n",
    "    img = img / 255.0\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a911903-099c-4b75-8eaf-8b1de87cf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_varias_imagenes(rutas_imagenes):\n",
    "    imgs = []\n",
    "    for ruta_imagen in rutas_imagenes:\n",
    "        img = cargar_y_preprocesar_imagen(ruta_imagen)\n",
    "        imgs.append(img)\n",
    "    # Crear un batch de imágenes\n",
    "    imgs_batch = tf.stack(imgs, axis=0)\n",
    "    # Hacer las predicciones\n",
    "    pred_probs = pruned_model.predict(imgs_batch)\n",
    "    pred_labels = np.argmax(pred_probs, axis=1)\n",
    "    pred_class_names = label_encoder.inverse_transform(pred_labels)\n",
    "    # Mostrar las imágenes con sus predicciones\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(len(rutas_imagenes)):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(imgs[i])\n",
    "        plt.title(f'Predicción: {pred_class_names[i]}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Imprimir las predicciones\n",
    "    for i, ruta_imagen in enumerate(rutas_imagenes):\n",
    "        print(f'Imagen: {ruta_imagen} - Predicción: {pred_class_names[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b1b5a-f6d8-466d-883c-3e5c744231a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de rutas de imágenes\n",
    "rutas_imagenes = [\n",
    "    'PokemonTest/14564190248867.png',\n",
    "    'PokemonTest/800px-Gengar.png',\n",
    "    'PokemonTest/images.jpg',\n",
    "    'PokemonTest/1.jpg',\n",
    "    # Agrega más rutas según necesites\n",
    "]\n",
    "\n",
    "# Hacer las predicciones\n",
    "predecir_varias_imagenes(rutas_imagenes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d82b8-a8a0-436f-a370-0d38283018e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cnn_env)",
   "language": "python",
   "name": "cnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
