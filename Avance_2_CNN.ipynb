{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc0c48f-d18d-499c-a310-f9bf63473049",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfmt\u001b[39;00m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/__init__.py:86\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# To ensure users only access the expected public API, the API structure is\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# created in the `api` directory. Import all api modules.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Import API modules for Tensorflow Model Optimization.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Module containing code for clustering.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/api/clustering/keras/__init__.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_scope\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_weights\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m strip_clustering\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_config\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_wrapper\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_centroids\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/cluster_wrapper.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_config\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clusterable_layer\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_centroids\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_registry\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/clustering/keras/clustering_centroids.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_ops\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_config\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     25\u001b[0m k \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mbackend\n\u001b[1;32m     26\u001b[0m CentroidInitialization \u001b[38;5;241m=\u001b[39m cluster_config\u001b[38;5;241m.\u001b[39mCentroidInitialization\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/keras/compat.py:41\u001b[0m\n\u001b[1;32m     37\u001b[0m     keras_internal \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m keras_internal\n\u001b[0;32m---> 41\u001b[0m keras \u001b[38;5;241m=\u001b[39m \u001b[43m_get_keras_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21massign\u001b[39m(ref, value, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massign\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow_model_optimization/python/core/keras/compat.py:33\u001b[0m, in \u001b[0;36m_get_keras_instance\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_USE_LEGACY_KERAS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Use Keras 2.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m version_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mversion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_fn \u001b[38;5;129;01mand\u001b[39;00m version_fn()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_internal\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,unused-import\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:210\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule\u001b[38;5;241m.\u001b[39mstartswith(\n\u001b[1;32m    205\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.legacy.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m   ):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not available with Keras 3.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[0;32m--> 210\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, item)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:52\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:210\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule\u001b[38;5;241m.\u001b[39mstartswith(\n\u001b[1;32m    205\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.legacy.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m   ):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not available with Keras 3.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[0;32m--> 210\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, item)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:52\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Emit a warning if one was specified\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: KerasLazyLoader.__getattr__ at line 210 (4948 times), LazyLoader._load at line 52 (4948 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:210\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule\u001b[38;5;241m.\u001b[39mstartswith(\n\u001b[1;32m    205\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.legacy.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m   ):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not available with Keras 3.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[0;32m--> 210\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, item)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_env/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:52\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Emit a warning if one was specified\u001b[39;00m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os \n",
    "import tensorflow_model_optimization as tfmt \n",
    "import tensorflow as tf\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import zipfile\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras import layers \n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError\n",
    "from tqdm import tqdm  \n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20797043-e4bc-46f5-a511-33384fbfb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(\"TF MOT version:\", tfmt.__version__)\n",
    "print(\"TF KERAS\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0372e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Python executable:\", sys.executable)            # Ruta del Python en uso\n",
    "print(\"TensorFlow version:\", tf.__version__)           # Asegúrate sea >=2.9 (ideal 2.10+)\n",
    "print(\"TF MOT version:\", tfmt.__version__)            # Al menos 0.7.0, preferible 0.8.0\n",
    "print(\"Eager mode:\", tf.executing_eagerly())           # Debe ser True en TF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a104bb-9735-4a99-ba2d-b335cd28f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b04d07d-08c4-4f1e-8569-9cd56070ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Preprocesamiento del Dataset\n",
    "# Definir el directorio donde se encuentran las imágenes del dataset\n",
    "with zipfile.ZipFile('/home/pibezx/Documents/CNN/pokemon.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('pokemos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "297346d8-4fc9-4424-9359-ed247f3921b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"pokemos/PokemonData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd9ad4-cc7e-4e3b-8086-ce99a38aa148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Preprocesamiento del Dataset\n",
    "# Parámetros de preprocesamiento\n",
    "IMG_SIZE = (224, 224)  # Tamaño al que se redimensionarán las imágenes\n",
    "BATCH_SIZE = 32  # Tamaño del lote para preprocesamiento\n",
    "\n",
    "# Listar todas las imágenes en el directorio\n",
    "image_files = []\n",
    "labels = []\n",
    "for root, _, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "            image_files.append(os.path.join(root, file))\n",
    "            labels.append(os.path.basename(root))\n",
    "\n",
    "# Convertir listas de archivos y etiquetas a tensores\n",
    "image_files = tf.constant(image_files)\n",
    "labels = tf.constant(labels)\n",
    "\n",
    "# Filtrar clases con al menos 2 ejemplos\n",
    "label_counts = Counter(labels.numpy())\n",
    "valid_labels = [label for label, count in label_counts.items() if count >= 2]\n",
    "filtered_image_files = []\n",
    "filtered_labels = []\n",
    "for img, label in zip(image_files.numpy(), labels.numpy()):\n",
    "    if label in valid_labels:\n",
    "        filtered_image_files.append(img)\n",
    "        filtered_labels.append(label)\n",
    "\n",
    "image_files = tf.constant(filtered_image_files)\n",
    "labels = tf.constant(filtered_labels)\n",
    "\n",
    "print(f\"Total de imágenes después de filtrar: {len(image_files)}\")\n",
    "\n",
    "\n",
    "# Codificar las etiquetas para que sean valores enteros\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels.numpy())\n",
    "\n",
    "# Convertir etiquetas a tensor de TensorFlow\n",
    "labels = tf.constant(labels)\n",
    "\n",
    "# Crear un dataset de TensorFlow con las rutas de las imágenes y etiquetas\n",
    "def parse_function(filename, label):\n",
    "    # Leer el archivo de imagen\n",
    "    img = tf.io.read_file(filename)\n",
    "    # Decodificar la imagen\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Redimensionar la imagen\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    # Normalizar los valores de los píxeles entre 0 y 1\n",
    "    img = img / 255.0\n",
    "    return img, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_files, labels))\n",
    "dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#dataset = dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(f\"Número de imágenes cargadas inicialmente: {len(image_files)}\")\n",
    "print(f\"Número de etiquetas cargadas inicialmente: {len(labels)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c343014-5ffb-4261-8509-ec4be152adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: División del Dataset\n",
    "print(\"Dividiendo el dataset en conjuntos de entrenamiento, validación y prueba...\")\n",
    "dataset = dataset.shuffle(len(image_files), reshuffle_each_iteration=False)\n",
    "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "print(f\"Número total de muestras en el dataset: {dataset_size}\")\n",
    "train_size = int(0.7 * len(image_files))\n",
    "val_size = int(0.2 * len(image_files))\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "print(f\"Tamaño del conjunto de Entrenamiento: {train_size}\")\n",
    "print(f\"Tamaño del conjunto de Validación: {val_size}\")\n",
    "print(f\"Tamaño del conjunto de Prueba: {test_size}\")\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_test_dataset = dataset.skip(train_size)\n",
    "val_dataset = val_test_dataset.take(val_size)\n",
    "test_dataset = val_test_dataset.skip(val_size)\n",
    "\"\"\"\n",
    "# Definir la función de data augmentation\n",
    "def augment(image, label):\n",
    "    # Aplicar transformaciones aleatorias\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n",
    "    image = tf.image.random_hue(max_delta=0.1)\n",
    "    # Asegurarse de que la imagen sigue en el rango [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    return image, label\n",
    "\n",
    "# Aplicar data augmentation solo al conjunto de entrenamiento\n",
    "train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\"\"\"\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Tamaño del conjunto de Entrenamiento (número de lotes): {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
    "print(f\"Tamaño del conjunto de Validación (número de lotes): {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
    "print(f\"Tamaño del conjunto de Prueba (número de lotes): {tf.data.experimental.cardinality(test_dataset).numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c16c4-65b2-4bed-ab98-71c4533b4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Entrenamiento del Modelo con MobileNetV2\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "print(\"Configurando el modelo MobileNetV2 para entrenamiento...\")\n",
    "#input_tensor = Input(shape=(224, 224, 3))  # Usando 3 canales ya que las imágenes son ahora \"RGB\"\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76965e3a-d76e-4318-a3b2-ddee2b09ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# -----------\n",
    "# Paso 2: (Opcional) Congelar las capas de la base\n",
    "# -----------\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,               # Base MobileNetV2 podada\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(valid_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# -----------\n",
    "# Paso 3: Definir parámetros para pruning\n",
    "# -----------\n",
    "epochs = 40\n",
    "steps_per_epoch =  np.ceil(train_size / BATCH_SIZE) # Ajusta según tu dataset\n",
    "\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmt.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=0.6,\n",
    "        begin_step=0,\n",
    "        end_step=steps_per_epoch * epochs\n",
    "    )\n",
    "}\n",
    "\n",
    "# -----------\n",
    "# Paso 4: Envolver la base en prune_low_magnitude\n",
    "\n",
    "print(\"model type:\", type(model))\n",
    "print(\"model module:\", type(model).__module__)\n",
    "print(\"model qualname:\", type(model).__qualname__)\n",
    "# -----------\n",
    "pruned_model= tfmt.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# -----------\n",
    "# Paso 6: Compilar\n",
    "# -----------\n",
    "pruned_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -----------\n",
    "# Paso 7: Entrenar con callbacks para pruning y early stopping\n",
    "# -----------\n",
    "callbacks = [\n",
    "    tfmt.sparsity.keras.UpdatePruningStep(),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = pruned_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\"\"\"\n",
    "# Parámetros para pruning\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmt.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0, \n",
    "        final_sparsity=0.6, \n",
    "        begin_step=0, \n",
    "        end_step=steps_per_epoch * epochs\n",
    "    )\n",
    "}\n",
    "prune_low_magnitude = tfmt.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# === Paso 1: Envolver la base de MobileNetV2 con prune_low_magnitude ===\n",
    "pruned_model_base = prune_low_magnitude(base_model, **pruning_params)\n",
    "\n",
    "# === Paso 2: Construir la parte final de la red usando la salida de la red podada ===\n",
    "x = pruned_model_base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(len(valid_labels), activation='softmax')(x)\n",
    "\n",
    "# === Paso 3: Crear el modelo que realmente usaremos, con pruning incluido ===\n",
    "pruned_model = Model(inputs=pruned_model_base.input, outputs=predictions)\n",
    "\n",
    "# Compilar\n",
    "pruned_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "pruning_callback = tfmt.sparsity.keras.UpdatePruningStep()\n",
    "\n",
    "# Entrenar\n",
    "pruned_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, pruning_callback]\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1aeda-2fe8-4236-b839-d2ad13a7461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = pruned_model.evaluate(test_dataset)\n",
    "\n",
    "# Imprimir la precisión en el conjunto de prueba\n",
    "print(f\"Precisión en el conjunto de prueba: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6762165-a812-43fd-aee4-a3fc26b06be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Obtener las etiquetas verdaderas y predichas\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    predictions = pruned_model.predict(images)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(predicted_labels)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Obtener los nombres de las clases y decodificarlos si es necesario\n",
    "class_names = label_encoder.classes_\n",
    "class_names = [name.decode('utf-8') if isinstance(name, bytes) else name for name in class_names]\n",
    "\n",
    "# Número máximo de clases a visualizar\n",
    "max_classes = 20\n",
    "\n",
    "# Índices de las clases que deseas visualizar (por ejemplo, las primeras 20)\n",
    "clases_mostrar = np.arange(max_classes)\n",
    "\n",
    "# Obtener los nombres de las clases de la lista ya decodificada\n",
    "class_names_mostrar = [class_names[i] for i in clases_mostrar]\n",
    "\n",
    "\n",
    "# Normalizar la matriz de confusión completa\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Extraer la submatriz para las clases seleccionadas\n",
    "cm_mostrar = cm_norm[np.ix_(clases_mostrar, clases_mostrar)]\n",
    "\n",
    "# Visualizar la matriz de confusión normalizada para las clases seleccionadas\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_mostrar, annot=False, cmap='Blues', xticklabels=class_names_mostrar, yticklabels=class_names_mostrar)\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.title('Matriz de Confusión Normalizada (20 Clases Seleccionadas)')\n",
    "plt.show()\n",
    "\n",
    "# Generar el reporte de clasificación para las clases seleccionadas\n",
    "report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    labels=clases_mostrar, \n",
    "    target_names=class_names_mostrar\n",
    ")\n",
    "\n",
    "print(\"Reporte de Clasificación para las Clases Seleccionadas:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da2dc4-5e27-455c-a875-4e8d0f347f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_predicciones_en_prueba(dataset, num_images=6):\n",
    "    # Crear un iterador del dataset\n",
    "    dataset_iter = iter(dataset.unbatch().shuffle(1000))\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_images):\n",
    "        # Obtener la imagen y la etiqueta verdadera\n",
    "        image, label = next(dataset_iter)\n",
    "        # Expandir dimensiones para predecir\n",
    "        image_expanded = tf.expand_dims(image, axis=0)\n",
    "        # Hacer la predicción\n",
    "        pred_probs = pruned_model.predict(image_expanded)\n",
    "        pred_label = np.argmax(pred_probs, axis=1)[0]\n",
    "        # Obtener los nombres de las clases\n",
    "        true_class_name = label_encoder.inverse_transform([label.numpy()])[0]\n",
    "        pred_class_name = label_encoder.inverse_transform([pred_label])[0]\n",
    "        # Mostrar la imagen con las etiquetas\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'Verdadero: {true_class_name}\\nPredicción: {pred_class_name}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Mostrar predicciones en imágenes del conjunto de prueba\n",
    "mostrar_predicciones_en_prueba(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cc76b-e924-4572-ab25-9f446ca08774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_y_preprocesar_imagen(ruta_imagen, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Carga una imagen desde ruta_imagen, la decodifica, la redimensiona\n",
    "    y la normaliza a [0.0, 1.0].\n",
    "    Devuelve un tf.Tensor con la forma (224, 224, 3).\n",
    "    \"\"\"\n",
    "    # 1. Leer la imagen en binario\n",
    "    img = tf.io.read_file(ruta_imagen)\n",
    "\n",
    "    # 2. Decodificar la imagen (asumiendo JPG/PNG con 3 canales)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "    # 3. Redimensionar la imagen\n",
    "    img = tf.image.resize(img, img_size)\n",
    "\n",
    "    # 4. Normalizar la imagen [0,1]\n",
    "    img = img / 255.0\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a911903-099c-4b75-8eaf-8b1de87cf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_varias_imagenes(rutas_imagenes):\n",
    "    imgs = []\n",
    "    for ruta_imagen in rutas_imagenes:\n",
    "        img = cargar_y_preprocesar_imagen(ruta_imagen)\n",
    "        imgs.append(img)\n",
    "    # Crear un batch de imágenes\n",
    "    imgs_batch = tf.stack(imgs, axis=0)\n",
    "    # Hacer las predicciones\n",
    "    pred_probs = pruned_model.predict(imgs_batch)\n",
    "    pred_labels = np.argmax(pred_probs, axis=1)\n",
    "    pred_class_names = label_encoder.inverse_transform(pred_labels)\n",
    "    # Mostrar las imágenes con sus predicciones\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(len(rutas_imagenes)):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(imgs[i])\n",
    "        plt.title(f'Predicción: {pred_class_names[i]}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Imprimir las predicciones\n",
    "    for i, ruta_imagen in enumerate(rutas_imagenes):\n",
    "        print(f'Imagen: {ruta_imagen} - Predicción: {pred_class_names[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b1b5a-f6d8-466d-883c-3e5c744231a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de rutas de imágenes\n",
    "rutas_imagenes = [\n",
    "    'PokemonTest/14564190248867.png',\n",
    "    'PokemonTest/800px-Gengar.png',\n",
    "    'PokemonTest/images.jpg',\n",
    "    'PokemonTest/1.jpg',\n",
    "    # Agrega más rutas según necesites\n",
    "]\n",
    "\n",
    "# Hacer las predicciones\n",
    "predecir_varias_imagenes(rutas_imagenes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d82b8-a8a0-436f-a370-0d38283018e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
